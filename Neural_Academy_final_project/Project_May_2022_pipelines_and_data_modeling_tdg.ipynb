{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b5ae1e",
   "metadata": {},
   "source": [
    "## Pipelines and data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9dde1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in the system: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "  \n",
    "n_cpu = os.cpu_count()\n",
    "print(\"Number of CPUs in the system:\", n_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008bdf88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters optimization method: GridSearchCV\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   2.4s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   0.8s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total=  38.6s\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   0.7s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   1.7s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   3.1s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   0.7s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total=  35.1s\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   0.5s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   1.5s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   5.9s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   1.6s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total= 1.2min\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   1.6s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   2.9s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   3.2s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   0.9s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total=  36.3s\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   0.8s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   1.5s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   3.0s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   0.8s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total=  35.6s\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   0.7s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   1.5s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   5.9s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   1.5s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total= 1.2min\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   1.7s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   3.6s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   3.5s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   0.9s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total=  35.7s\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   0.7s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   1.4s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   4.1s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   0.7s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total=  36.0s\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   0.9s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   2.2s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   6.3s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   1.6s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total= 1.2min\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   1.7s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   3.4s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   3.1s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   0.6s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total=  36.6s\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   0.8s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   1.7s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   3.0s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   0.7s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total=  34.2s\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   0.7s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   1.5s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   5.2s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   1.6s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total= 1.2min\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   1.9s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   2.9s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   2.6s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   0.8s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total=  35.8s\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   0.8s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   1.4s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   2.8s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   0.7s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total=  35.4s\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   0.6s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   1.6s\n",
      "[Pipeline] ............... (step 1 of 5) Processing s2f, total=   5.8s\n",
      "[Pipeline] ................ (step 2 of 5) Processing sc, total=   1.5s\n",
      "[Pipeline] ........... (step 3 of 5) Processing outlier, total= 1.2min\n",
      "[Pipeline] ................ (step 4 of 5) Processing us, total=   1.5s\n",
      "[Pipeline] ............... (step 5 of 5) Processing clf, total=   3.2s\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   5.5s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.5s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 4.2min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   5.2s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.5s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 4.1min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   5.3s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   2.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 4.4min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   5.5s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.3s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 4.2min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   4.9s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   2.3s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 4.3min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   3.3s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   0.9s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=  57.6s\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   2.9s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 1.1min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   5.6s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.9s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 2.5min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   2.5s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   0.9s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 1.1min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   3.1s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   0.9s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 1.1min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   5.5s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.8s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 2.5min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   3.5s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 1.1min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   2.8s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   0.9s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 1.1min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   6.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.8s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 2.5min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   3.0s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   0.9s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 1.1min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   3.2s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   0.7s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 1.1min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   6.3s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.6s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 2.5min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   3.3s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.1s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 1.2min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   3.0s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   0.8s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 1.2min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   6.4s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   2.1s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 2.5min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pipe_lr</th>\n",
       "      <td>68.89</td>\n",
       "      <td>75.46</td>\n",
       "      <td>67.86</td>\n",
       "      <td>68.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pipe_ab</th>\n",
       "      <td>72.02</td>\n",
       "      <td>79.29</td>\n",
       "      <td>71.16</td>\n",
       "      <td>71.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pipe_xgb</th>\n",
       "      <td>91.87</td>\n",
       "      <td>97.44</td>\n",
       "      <td>91.77</td>\n",
       "      <td>91.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          accuracy  roc_auc  precision  recall\n",
       "Model                                         \n",
       "pipe_lr      68.89    75.46      67.86   68.50\n",
       "pipe_ab      72.02    79.29      71.16   71.42\n",
       "pipe_xgb     91.87    97.44      91.77   91.50"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "from sklearn.feature_selection import RFE, VarianceThreshold\n",
    "import pickle\n",
    "import joblib\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import set_config\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn import FunctionSampler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Custom Transformers\n",
    "###############################################################################################################################\n",
    "\n",
    "# Logger class for debugging\n",
    "\n",
    "class Logger:\n",
    "    \n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        print(\"### Logger (fit): \" + self.message)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(\"### Logger (transform)\" + self.message)\n",
    "        return X\n",
    "\n",
    "###############################################################################################################################\n",
    "\n",
    "#  Processing categorical variables (Feature Engineering)\n",
    "\n",
    "# get_dummies like\n",
    "\n",
    "class GetDummies(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, dummy_columns):\n",
    "        self.columns = None\n",
    "        self.dummy_columns = dummy_columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = pd.get_dummies(X, columns=self.dummy_columns).columns # learned column names\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_new = pd.get_dummies(X, columns=self.dummy_columns)\n",
    "        return X_new.reindex(columns=self.columns, fill_value=0)\n",
    "    \n",
    "    \n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# for binary problem\n",
    "\n",
    "class ReplaceColumns(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.unique_values = {}\n",
    "        self.n_cat = {}\n",
    "        \n",
    "        for col in self.columns:\n",
    "            self.unique_values[col] = list(X[col].unique())\n",
    "            self.n_cat[col] = list(np.arange(len(self.unique_values[col])))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        for col in self.columns:\n",
    "            \n",
    "            X[col] = X[col].replace(self.unique_values[col], self.n_cat[col], inplace = False)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "\n",
    "# for n_cat > 2    \n",
    "\n",
    "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.unique_values = {}\n",
    "        \n",
    "        for col in self.columns:\n",
    "            self.unique_values[col] = list(X[col].unique())\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        for col in self.columns:\n",
    "            for cat in self.unique_values[col]:\n",
    "                X[col + '_' + str(cat)] = np.where(X[col] == cat, 1, 0)\n",
    "            \n",
    "            X = X.drop(columns = [col], axis = 1)\n",
    "        \n",
    "        return X\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "# OrdinalEncoder\n",
    "\n",
    "class OrdinalEncoderColumns(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.oe = {}\n",
    "        x = {}\n",
    "        \n",
    "        for col in self.columns:\n",
    "            \n",
    "            x[col] = X[col].values.reshape(-1, 1)\n",
    "            self.oe[col] = OrdinalEncoder(categories = 'auto', handle_unknown='use_encoded_value', unknown_value = -1)\n",
    "            self.oe[col].fit(x[col])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        for col in self.columns:\n",
    "            \n",
    "            X[col] = self.oe[col].transform(X[col].values.reshape(-1, 1))\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class GetFirstLetter(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        for col in self.columns:\n",
    "            X[col] = X[col].astype(str).str[0]\n",
    "            \n",
    "        return X\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class String2UniqueFeature(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, column, str_leng):\n",
    "        self.column = column\n",
    "        self.str_leng = str_leng\n",
    "        \n",
    "    def fit(self, X, y):    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        for i in range(self.str_leng):\n",
    "            X[f'ch{i}'] = X[self.column].str.get(i).apply(ord)\n",
    "        \n",
    "        X[\"unique_characters\"] = X[self.column].apply(lambda x: len(set(x)))\n",
    "        \n",
    "        X = X.drop(columns = [self.column], axis = 1)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "###############################################################################################################################\n",
    "    \n",
    "# Missing values\n",
    "\n",
    "class SimpleImputers(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns, strategy, category = None):\n",
    "        self.columns = columns # warning: columns deve essere una lista\n",
    "        self.strategy = strategy\n",
    "        self.category = category\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.new_value = {} # storage learned value for each column\n",
    "        \n",
    "        for col in self.columns:\n",
    "            \n",
    "            if self.strategy == 'mean':\n",
    "                self.new_value[col] = X[col].mean()\n",
    "            \n",
    "            if self.strategy == 'median':\n",
    "                self.new_value[col] = X[col].median()\n",
    "                \n",
    "            if self.strategy == 'new_category':\n",
    "                self.new_value[col] = self.category\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \n",
    "        for col in self.columns:\n",
    "            X[col] = X[col].fillna(self.new_value[col])\n",
    "    \n",
    "        return X\n",
    "\n",
    "\n",
    "class InvalidValueImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns, value = -1, suffix = \"_invalid\"):\n",
    "        self.columns = columns\n",
    "        self.value = value\n",
    "        self.suffix = suffix\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        for col in self.columns:\n",
    "            X[col + self.suffix] = np.where(X[col].is_null(), 1, 0)\n",
    "            X[col] = X[col].fillna(self.value)\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    \n",
    "class ModelPredictImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator, target_name, feature_names):\n",
    "        \n",
    "        self.estimator = estimator\n",
    "        self.target_name = target_name\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "    \n",
    "        bool_target_not_null = X[self.target_name].notnull()\n",
    "\n",
    "        row_indexes_where_target_notnull = X.index[np.where(bool_target_not_null)]\n",
    "\n",
    "        X_train = X.loc[row_indexes_where_target_notnull, self.feature_names]\n",
    "        y_train = X.loc[row_indexes_where_target_notnull, self.target_name]\n",
    "    \n",
    "        self.ist = self.estimator.fit(X_train, y_train) \n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \n",
    "        bool_target_is_null = X[self.target_name].isnull()\n",
    "        row_indexes_where_target_isnull = X.index[np.where(bool_target_is_null)]\n",
    "        X_test = X.loc[row_indexes_where_target_isnull, self.feature_names]\n",
    "        \n",
    "        try:\n",
    "            preds = self.ist.predict(X_test)\n",
    "            X.loc[row_indexes_where_target_isnull, self.target_name] = preds\n",
    "        \n",
    "        except:\n",
    "            X = X\n",
    "    \n",
    "        return X\n",
    "\n",
    "\n",
    "    \n",
    "###############################################################################################################################   \n",
    "    \n",
    "# Normalization\n",
    "        \n",
    "class ScalerColumns(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, scaler): # class inputs: columns list & scaler\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.sc = {}\n",
    "        x = {}\n",
    "        \n",
    "        for col in X.columns:\n",
    "            \n",
    "\n",
    "            x[col] = X[col].values.reshape(-1, 1) # returns a numpy array\n",
    "\n",
    "            if self.scaler == 'MinMaxScaler':\n",
    "                self.sc[col] = MinMaxScaler()\n",
    "                self.sc[col].fit(x[col])\n",
    "\n",
    "            if self.scaler == 'StandardScaler':\n",
    "                self.sc[col] = StandardScaler()\n",
    "                self.sc[col].fit(x[col])\n",
    "                \n",
    "            if self.scaler == 'RobustScaler':\n",
    "                self.sc[col] = RobustScaler()\n",
    "                self.sc[col].fit(x[col])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        for col in X.columns:\n",
    "            \n",
    "            X[col] = self.sc[col].transform(X[col].values.reshape(-1, 1))\n",
    "        \n",
    "        return X\n",
    "        \n",
    "############################################################################################################################### \n",
    "\n",
    "# Outlier Imputing\n",
    "\n",
    "class OutlierColumnsImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.q1 = {} \n",
    "        self.q3 = {} \n",
    "        self.med = {}\n",
    "        self.iqr= {}\n",
    "        self.upper_bound = {}\n",
    "        self.lower_bound = {}\n",
    "        \n",
    "        for col in self.columns:\n",
    "            \n",
    "            self.q1[col] = np.quantile(X[col], 0.25)\n",
    "            self.q3[col] = np.quantile(X[col], 0.75)\n",
    "            self.med[col] = np.median(X[col])\n",
    "\n",
    "            self.iqr[col] = self.q3[col] - self.q1[col]\n",
    "\n",
    "            self.upper_bound[col] = self.q3[col] + (1.5 * self.iqr[col])\n",
    "            self.lower_bound[col] = self.q1[col] - (1.5 * self.iqr[col])\n",
    "        \n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        \n",
    "        new_X = X.copy()\n",
    "        \n",
    "        for col in self.columns:\n",
    "            # capping\n",
    "            new_X[col] = np.where(new_X[col] > self.upper_bound[col], self.upper_bound[col], new_X[col])\n",
    "            new_X[col] = np.where(new_X[col] < self.lower_bound[col], self.lower_bound[col], new_X[col])\n",
    "            \n",
    "        return new_X\n",
    "\n",
    "\n",
    "###############################################################################################################################\n",
    "    \n",
    "# Feature Selection\n",
    "\n",
    "class FsCorrMatrix(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n, redundancy_threshold):\n",
    "        self.n = n                                           # n (int) number of features to select\n",
    "        self.redundancy_threshold = redundancy_threshold\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        df_num = pd.concat([y, X], axis=1)\n",
    "        corr_matrix_abs = df_num.corr().abs()\n",
    "        \n",
    "        # upper_tri = upper triangular correlation matrix of features\n",
    "        \n",
    "        upper_tri = corr_matrix_abs.where(np.triu(np.ones(corr_matrix_abs.shape), k=1).astype(bool))\n",
    "        upper_tri = upper_tri.drop(y.name, axis=0).drop(y.name, axis=1)\n",
    "        \n",
    "        # to_drop = list of redundant highly correlated features\n",
    "        \n",
    "        to_drop = [col_name for col_name in upper_tri.columns if any(upper_tri[col_name] > self.redundancy_threshold)]\n",
    "        \n",
    "        corr_matrix_abs.drop(corr_matrix_abs[to_drop], axis=0, inplace=True) \n",
    "        \n",
    "        correlation_y = corr_matrix_abs[y.name]\n",
    "        correlation_y_sorted = correlation_y.sort_values(ascending = False)\n",
    "        fs = correlation_y_sorted[1:(self.n+1)]  # 1 to skip y itself\n",
    "        self.index = fs.index\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X[self.index]\n",
    "        return X\n",
    "\n",
    "\n",
    "###############################################################################################################################\n",
    "# Processing X and y in Pipeline/ remove/add rows -> FunctionSampler from Imblearn\n",
    "\n",
    "# Outlier rejection -> FunctionSampler in pipeline\n",
    "\n",
    "def outlier_rejection_with_model(X, y, max_samples='auto', contamination='auto'):\n",
    "    model = IsolationForest(max_samples=max_samples, contamination=contamination)\n",
    "    model.fit(X)\n",
    "    y_pred = model.predict(X)  # 1 if good, -1 if outlier\n",
    "    X_good = X[y_pred == 1]\n",
    "    y_good = y[y_pred == 1]\n",
    "    return X_good, y_good\n",
    "\n",
    "# Drop NaN values in y target (funzione da usare, nel caso, prima della pipeline)\n",
    "\n",
    "def drop_y_na(X, y):\n",
    "    # Not usable, as sklearn consider invalid \"y\" with NaN\n",
    "    y_good_index = np.where(pd.notna(y))[0]\n",
    "    X_good = X[y_good_index]\n",
    "    y_good = y[y_good_index]\n",
    "    return X_good, y_good\n",
    "\n",
    "# Column names: remove white spaces and convert to lower case\n",
    "\n",
    "def columns_strip_lower(X, y):\n",
    "    \n",
    "    X.columns = X.columns.str.strip().str.lower()\n",
    "    y.name = y.name.strip().lower()\n",
    "    return X, y\n",
    "\n",
    "###############################################################################################################################\n",
    "    \n",
    "# Data Loading\n",
    "\n",
    "df_raw = pd.read_csv('train.csv')\n",
    "df = df_raw.copy()\n",
    "X = df.drop(['id','target'], axis = 1)\n",
    "y = df['target']\n",
    "df.head()\n",
    "\n",
    "###############################################################################################################################\n",
    "\n",
    "# Judge with Nested-CrossValidation & pipelines\n",
    "\n",
    "class Judge():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def set_data(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        return self\n",
    "    \n",
    "    def set_pipelines(self, pipelines):\n",
    "        self.pipelines = pipelines\n",
    "        return self\n",
    "    \n",
    "    def set_metrics(self, metrics):\n",
    "        self.metrics = metrics\n",
    "        return self\n",
    "    \n",
    "    def set_models(self, models):\n",
    "        self.models = models\n",
    "        return self\n",
    "    \n",
    "    def set_nested_cv(self, tuning_method, hpars, inner_cv = None, outer_cv = None, rscv_random_state = None):\n",
    "        self.tuning_method = tuning_method\n",
    "        self.hpars = hpars\n",
    "        self.inner_cv = inner_cv\n",
    "        self.outer_cv = outer_cv\n",
    "        self.rscv_random_state = rscv_random_state \n",
    "        return self\n",
    "    \n",
    "    def __get_performance_from_algorithm(self):\n",
    "        \n",
    "        print('Hyper-parameters optimization method: {}'.format(self.tuning_method))\n",
    "        \n",
    "        # Matrix score\n",
    "        matrix_score = np.array(np.zeros(len(self.models)*len(self.metrics))).reshape(len(self.models), len(self.metrics))\n",
    "            \n",
    "        for i, (k, v) in enumerate(self.pipelines.items()):\n",
    "\n",
    "            pipe = v \n",
    "\n",
    "            # Hyper-params optimization\n",
    "\n",
    "            if self.tuning_method == 'GridSearchCV':\n",
    "\n",
    "                try:\n",
    "                    clf = GridSearchCV(estimator=pipe, param_grid=self.hpars[k], cv=self.inner_cv) \n",
    "                except:\n",
    "                    clf = pipe # Default pipe\n",
    "\n",
    "            elif self.tuning_method == 'RandomizedSearchCV':\n",
    "                try:\n",
    "                    clf = RandomizedSearchCV(estimator=pipe, param_distributions=self.hpars[k], cv=self.inner_cv, \n",
    "                                             random_state=self.rscv_random_state) \n",
    "                except:\n",
    "                    clf = pipe # Default pipe\n",
    "\n",
    "            # Nested-CV scores\n",
    "            cv_results = cross_validate(clf, self.X, self.y, scoring = self.metrics, cv=self.outer_cv)\n",
    "            for j, metric in enumerate(self.metrics):\n",
    "                matrix_score[i,j] = round(cv_results['test_' + metric].mean()*100, 2)\n",
    "\n",
    "        return matrix_score\n",
    "    \n",
    "    def get_table(self):\n",
    "        \n",
    "        matrix_score = self.__get_performance_from_algorithm()\n",
    "        tab = pd.DataFrame(matrix_score, columns = self.metrics).set_axis(self.models).rename_axis('Model')\n",
    "        return tab\n",
    "    \n",
    "    @staticmethod\n",
    "    def info_class():\n",
    "        \n",
    "        info = print(\"\"\"\n",
    "        \n",
    "        class name -> Judge\n",
    "        \n",
    "        -methods\n",
    "        \n",
    "        set_data -> X = features, y = targets\n",
    "        set_pipelines -> pipelines (dict)\n",
    "        set_metrics -> metrics (string list)\n",
    "        set_models -> models (string list)\n",
    "        get_table -> return DataFrame with evaluated metrics for each model\n",
    "        \n",
    "        -class parameters\n",
    "        \n",
    "        tuning_method -> 'GridSearchCV' or 'RandomizedSearchCV'\n",
    "        hpars -> hyperparameters input for the selected tuning_method (set hpars = {} for no nested-CV)\n",
    "        inner_cv -> hyperparameter tuning cross-validation splitting strategy\n",
    "        outer_cv -> model selection cross-validation splitting strategy\n",
    "        \n",
    "        \n",
    "        \"\"\")\n",
    "        \n",
    "        return info\n",
    "\n",
    "\n",
    "# # Judge settings\n",
    "\n",
    "random_state = 51\n",
    "rscv_random_state = 51\n",
    "\n",
    "pipelines = {\n",
    "    \n",
    "    'pipe_lr' : Pipeline(steps=[\n",
    "                ('s2f', String2UniqueFeature(column = 'f_27', str_leng = 10)),\n",
    "                ('sc', ScalerColumns(scaler='StandardScaler')),\n",
    "                ('outlier', FunctionSampler(func=outlier_rejection_with_model, kw_args={'max_samples': 'auto',\n",
    "                                                                                        'contamination': 'auto'})), \n",
    "                ('us', RandomUnderSampler()),\n",
    "                ('clf', LogisticRegression(random_state=random_state))\n",
    "                ], verbose = True),\n",
    "    \n",
    "    'pipe_ab' : Pipeline(steps=[\n",
    "                ('s2f', String2UniqueFeature(column = 'f_27', str_leng = 10)),\n",
    "                ('us', RandomUnderSampler()),\n",
    "                ('clf', AdaBoostClassifier(random_state=random_state))\n",
    "                ], verbose = True),\n",
    "    \n",
    "    'pipe_xgb' : Pipeline(steps=[\n",
    "                ('s2f', String2UniqueFeature(column = 'f_27', str_leng = 10)),\n",
    "                ('us', RandomUnderSampler()),\n",
    "                ('clf', XGBClassifier(use_label_encoder = False))\n",
    "                ], verbose = True),\n",
    "    \n",
    "}\n",
    "\n",
    "models = ['pipe_lr', 'pipe_ab', 'pipe_xgb']\n",
    "\n",
    "metrics= ['accuracy', 'roc_auc', 'precision', 'recall']\n",
    "\n",
    "tuning_method = 'GridSearchCV'\n",
    "# tuning_method = 'RandomizedSearchCV'\n",
    "\n",
    "grid_lr = {'clf__solver' : ['liblinear']}\n",
    "grid_xgb = {'clf__eval_metric' : ['logloss']}\n",
    "\n",
    "hpars= {'pipe_lr' : grid_lr, 'pipe_xgb' : grid_xgb}\n",
    "\n",
    "# hpars = {}\n",
    "\n",
    "# KFold_params\n",
    "inner_n_splits = 2 \n",
    "outer_n_splits = 5\n",
    "shuffle = True \n",
    "\n",
    "inner_cv = KFold(n_splits=inner_n_splits, shuffle=shuffle, random_state=random_state)\n",
    "outer_cv = KFold(n_splits=outer_n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "judge = Judge().set_data(X,y).set_pipelines(pipelines).set_metrics(metrics).set_models(models)\n",
    "judge.set_nested_cv(tuning_method, hpars, inner_cv, outer_cv)\n",
    "judge.get_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b5c00",
   "metadata": {},
   "source": [
    "### Production model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd5d3092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   5.9s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.4s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 2.5min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   6.1s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.4s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 2.5min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   6.4s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.4s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 2.5min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   6.1s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.4s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 2.5min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   6.4s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   1.8s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 2.5min\n",
      "[Pipeline] ............... (step 1 of 3) Processing s2f, total=   7.4s\n",
      "[Pipeline] ................ (step 2 of 3) Processing us, total=   2.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 3.2min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('s2f', String2UniqueFeature(column='f_27', str_leng=10)),\n",
       "                ('us', RandomUnderSampler()),\n",
       "                ('clf',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, enable_categorical=False,\n",
       "                               eval_metric='logloss', gamma=0, gpu_id=-1,\n",
       "                               importance_type=None, interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_delta_step=0,\n",
       "                               max_depth=6, min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
       "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=1, subsample=1,\n",
       "                               tree_method='exact', use_label_encoder=False,\n",
       "                               validate_parameters=1, verbosity=None))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_prod = GridSearchCV(pipelines['pipe_xgb'], hpars['pipe_xgb'])\n",
    "clf_prod.fit(X, y) # whole training-dataset\n",
    "clf_prod.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29b7c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "\n",
    "filename = 'may2022_kaggle_comp.pkl'\n",
    "joblib.dump(clf_prod, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfbd7029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 {color: black;background-color: white;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 pre{padding: 0;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-toggleable {background-color: white;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-item {z-index: 1;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-parallel-item:only-child::after {width: 0;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-383e171b-f1f9-4a7f-a342-f1634fca48b7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-383e171b-f1f9-4a7f-a342-f1634fca48b7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;s2f&#x27;, String2UniqueFeature(column=&#x27;f_27&#x27;, str_leng=10)),\n",
       "                (&#x27;us&#x27;, RandomUnderSampler()),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;,\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, enable_categorical=False,\n",
       "                               eval_metric=&#x27;logloss&#x27;, gamma=0, gpu_id=-1,\n",
       "                               importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "                               learning_rate=0.300000012, max_delta_step=0,\n",
       "                               max_depth=6, min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "                               n_jobs=8, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=1, subsample=1,\n",
       "                               tree_method=&#x27;exact&#x27;, use_label_encoder=False,\n",
       "                               validate_parameters=1, verbosity=None))],\n",
       "         verbose=True)</pre><b>Please rerun this cell to show the HTML repr or trust the notebook.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"01a86927-f133-4b0e-ab17-b44107bbdce7\" type=\"checkbox\" ><label for=\"01a86927-f133-4b0e-ab17-b44107bbdce7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;s2f&#x27;, String2UniqueFeature(column=&#x27;f_27&#x27;, str_leng=10)),\n",
       "                (&#x27;us&#x27;, RandomUnderSampler()),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;,\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, enable_categorical=False,\n",
       "                               eval_metric=&#x27;logloss&#x27;, gamma=0, gpu_id=-1,\n",
       "                               importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "                               learning_rate=0.300000012, max_delta_step=0,\n",
       "                               max_depth=6, min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "                               n_jobs=8, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=1, subsample=1,\n",
       "                               tree_method=&#x27;exact&#x27;, use_label_encoder=False,\n",
       "                               validate_parameters=1, verbosity=None))],\n",
       "         verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"22c483b8-be30-4fa0-8b04-b91490d01f9f\" type=\"checkbox\" ><label for=\"22c483b8-be30-4fa0-8b04-b91490d01f9f\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">String2UniqueFeature</label><div class=\"sk-toggleable__content\"><pre>String2UniqueFeature(column=&#x27;f_27&#x27;, str_leng=10)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b98eef88-ec00-4db6-80c9-bedd285cc4d4\" type=\"checkbox\" ><label for=\"b98eef88-ec00-4db6-80c9-bedd285cc4d4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomUnderSampler</label><div class=\"sk-toggleable__content\"><pre>RandomUnderSampler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a777dbdc-3c30-4aa0-9312-4907f0afb7a3\" type=\"checkbox\" ><label for=\"a777dbdc-3c30-4aa0-9312-4907f0afb7a3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              eval_metric=&#x27;logloss&#x27;, gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=&#x27;exact&#x27;, use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('s2f', String2UniqueFeature(column='f_27', str_leng=10)),\n",
       "                ('us', RandomUnderSampler()),\n",
       "                ('clf',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, enable_categorical=False,\n",
       "                               eval_metric='logloss', gamma=0, gpu_id=-1,\n",
       "                               importance_type=None, interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_delta_step=0,\n",
       "                               max_depth=6, min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
       "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=1, subsample=1,\n",
       "                               tree_method='exact', use_label_encoder=False,\n",
       "                               validate_parameters=1, verbosity=None))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline diagram\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "clf_prod.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c10204",
   "metadata": {},
   "source": [
    "### Datatest predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a20be53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>...</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.442517</td>\n",
       "      <td>0.174380</td>\n",
       "      <td>-0.999816</td>\n",
       "      <td>0.762741</td>\n",
       "      <td>0.186778</td>\n",
       "      <td>-1.074775</td>\n",
       "      <td>0.501888</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.006400</td>\n",
       "      <td>-1.193879</td>\n",
       "      <td>-2.435736</td>\n",
       "      <td>-2.427430</td>\n",
       "      <td>-1.966887</td>\n",
       "      <td>5.734205</td>\n",
       "      <td>BAAABADLAC</td>\n",
       "      <td>99.478419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.605598</td>\n",
       "      <td>-0.305715</td>\n",
       "      <td>0.627667</td>\n",
       "      <td>-0.578898</td>\n",
       "      <td>-1.750931</td>\n",
       "      <td>1.355550</td>\n",
       "      <td>-0.190911</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2.382405</td>\n",
       "      <td>0.149442</td>\n",
       "      <td>1.883322</td>\n",
       "      <td>-2.848714</td>\n",
       "      <td>-0.725155</td>\n",
       "      <td>3.194219</td>\n",
       "      <td>AFABBAEGCB</td>\n",
       "      <td>-65.993825</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.303990</td>\n",
       "      <td>2.445110</td>\n",
       "      <td>0.246515</td>\n",
       "      <td>0.818248</td>\n",
       "      <td>0.359731</td>\n",
       "      <td>-1.331845</td>\n",
       "      <td>1.358622</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.026098</td>\n",
       "      <td>1.312277</td>\n",
       "      <td>-5.157192</td>\n",
       "      <td>1.714005</td>\n",
       "      <td>0.585032</td>\n",
       "      <td>0.066898</td>\n",
       "      <td>BBACABBKEE</td>\n",
       "      <td>-87.405622</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.154053</td>\n",
       "      <td>0.260126</td>\n",
       "      <td>-1.367092</td>\n",
       "      <td>-0.093175</td>\n",
       "      <td>-1.111034</td>\n",
       "      <td>-0.948481</td>\n",
       "      <td>1.119220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.594532</td>\n",
       "      <td>-3.939475</td>\n",
       "      <td>1.754570</td>\n",
       "      <td>-2.364007</td>\n",
       "      <td>-1.003320</td>\n",
       "      <td>3.893099</td>\n",
       "      <td>AEBEAACQCC</td>\n",
       "      <td>-281.293460</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.651904</td>\n",
       "      <td>-0.424266</td>\n",
       "      <td>-0.667356</td>\n",
       "      <td>-0.322124</td>\n",
       "      <td>-0.089462</td>\n",
       "      <td>0.181705</td>\n",
       "      <td>1.784983</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084906</td>\n",
       "      <td>-0.985736</td>\n",
       "      <td>-0.130467</td>\n",
       "      <td>-3.557893</td>\n",
       "      <td>1.210687</td>\n",
       "      <td>1.861884</td>\n",
       "      <td>AEBBBBDABF</td>\n",
       "      <td>25.629415</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699995</th>\n",
       "      <td>0.640110</td>\n",
       "      <td>0.897808</td>\n",
       "      <td>-0.523956</td>\n",
       "      <td>1.563760</td>\n",
       "      <td>-0.092281</td>\n",
       "      <td>-0.610867</td>\n",
       "      <td>0.535426</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2.604048</td>\n",
       "      <td>1.122867</td>\n",
       "      <td>0.518110</td>\n",
       "      <td>1.243837</td>\n",
       "      <td>0.575111</td>\n",
       "      <td>0.076372</td>\n",
       "      <td>BCBCEBHMCD</td>\n",
       "      <td>204.186539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699996</th>\n",
       "      <td>-0.191771</td>\n",
       "      <td>-0.035246</td>\n",
       "      <td>-0.118533</td>\n",
       "      <td>0.584750</td>\n",
       "      <td>2.126977</td>\n",
       "      <td>0.568659</td>\n",
       "      <td>-0.052663</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.029857</td>\n",
       "      <td>1.384682</td>\n",
       "      <td>-1.135740</td>\n",
       "      <td>2.982713</td>\n",
       "      <td>-1.511760</td>\n",
       "      <td>2.225218</td>\n",
       "      <td>BAABCADQFC</td>\n",
       "      <td>-97.694591</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699997</th>\n",
       "      <td>-0.331704</td>\n",
       "      <td>-0.328845</td>\n",
       "      <td>-1.185503</td>\n",
       "      <td>1.022128</td>\n",
       "      <td>-0.483099</td>\n",
       "      <td>-0.107146</td>\n",
       "      <td>-0.968281</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4.021273</td>\n",
       "      <td>-1.845266</td>\n",
       "      <td>1.096011</td>\n",
       "      <td>-2.734508</td>\n",
       "      <td>-4.885955</td>\n",
       "      <td>-2.248739</td>\n",
       "      <td>AAAJCBGQBA</td>\n",
       "      <td>130.622745</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699998</th>\n",
       "      <td>-2.031073</td>\n",
       "      <td>-1.238398</td>\n",
       "      <td>0.964699</td>\n",
       "      <td>-1.045950</td>\n",
       "      <td>0.906064</td>\n",
       "      <td>0.634301</td>\n",
       "      <td>-0.707474</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.453864</td>\n",
       "      <td>-1.696606</td>\n",
       "      <td>1.018995</td>\n",
       "      <td>1.973697</td>\n",
       "      <td>-0.353068</td>\n",
       "      <td>-3.333449</td>\n",
       "      <td>BCBBCABNDE</td>\n",
       "      <td>-364.625148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699999</th>\n",
       "      <td>-0.085906</td>\n",
       "      <td>-0.002124</td>\n",
       "      <td>2.227375</td>\n",
       "      <td>0.217145</td>\n",
       "      <td>3.179153</td>\n",
       "      <td>-1.660188</td>\n",
       "      <td>0.891989</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.549082</td>\n",
       "      <td>-4.325318</td>\n",
       "      <td>-5.017221</td>\n",
       "      <td>0.251268</td>\n",
       "      <td>-3.236026</td>\n",
       "      <td>-0.362070</td>\n",
       "      <td>AFBEBACHFF</td>\n",
       "      <td>-155.417342</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
       "0       0.442517  0.174380 -0.999816  0.762741  0.186778 -1.074775  0.501888   \n",
       "1      -0.605598 -0.305715  0.627667 -0.578898 -1.750931  1.355550 -0.190911   \n",
       "2       0.303990  2.445110  0.246515  0.818248  0.359731 -1.331845  1.358622   \n",
       "3       0.154053  0.260126 -1.367092 -0.093175 -1.111034 -0.948481  1.119220   \n",
       "4      -1.651904 -0.424266 -0.667356 -0.322124 -0.089462  0.181705  1.784983   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "699995  0.640110  0.897808 -0.523956  1.563760 -0.092281 -0.610867  0.535426   \n",
       "699996 -0.191771 -0.035246 -0.118533  0.584750  2.126977  0.568659 -0.052663   \n",
       "699997 -0.331704 -0.328845 -1.185503  1.022128 -0.483099 -0.107146 -0.968281   \n",
       "699998 -2.031073 -1.238398  0.964699 -1.045950  0.906064  0.634301 -0.707474   \n",
       "699999 -0.085906 -0.002124  2.227375  0.217145  3.179153 -1.660188  0.891989   \n",
       "\n",
       "        f_07  f_08  f_09  ...      f_21      f_22      f_23      f_24  \\\n",
       "0          6     6     0  ... -1.006400 -1.193879 -2.435736 -2.427430   \n",
       "1          1     3     4  ...  2.382405  0.149442  1.883322 -2.848714   \n",
       "2          3     3     4  ... -7.026098  1.312277 -5.157192  1.714005   \n",
       "3          0     0     4  ... -0.594532 -3.939475  1.754570 -2.364007   \n",
       "4          2     2     2  ...  0.084906 -0.985736 -0.130467 -3.557893   \n",
       "...      ...   ...   ...  ...       ...       ...       ...       ...   \n",
       "699995     0     1     6  ...  2.604048  1.122867  0.518110  1.243837   \n",
       "699996     4     3     4  ...  3.029857  1.384682 -1.135740  2.982713   \n",
       "699997     1     1     2  ...  4.021273 -1.845266  1.096011 -2.734508   \n",
       "699998     5     1     1  ...  1.453864 -1.696606  1.018995  1.973697   \n",
       "699999     0     3     4  ... -3.549082 -4.325318 -5.017221  0.251268   \n",
       "\n",
       "            f_25      f_26        f_27        f_28  f_29  f_30  \n",
       "0      -1.966887  5.734205  BAAABADLAC   99.478419     0     0  \n",
       "1      -0.725155  3.194219  AFABBAEGCB  -65.993825     1     0  \n",
       "2       0.585032  0.066898  BBACABBKEE  -87.405622     0     1  \n",
       "3      -1.003320  3.893099  AEBEAACQCC -281.293460     0     0  \n",
       "4       1.210687  1.861884  AEBBBBDABF   25.629415     0     2  \n",
       "...          ...       ...         ...         ...   ...   ...  \n",
       "699995  0.575111  0.076372  BCBCEBHMCD  204.186539     0     0  \n",
       "699996 -1.511760  2.225218  BAABCADQFC  -97.694591     0     2  \n",
       "699997 -4.885955 -2.248739  AAAJCBGQBA  130.622745     1     0  \n",
       "699998 -0.353068 -3.333449  BCBBCABNDE -364.625148     0     0  \n",
       "699999 -3.236026 -0.362070  AFBEBACHFF -155.417342     0     1  \n",
       "\n",
       "[700000 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_raw = pd.read_csv('test.csv')\n",
    "df_test = df_test_raw.copy()\n",
    "X_test = df_test.drop(columns = ['id'], axis=1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d4fac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "preds_array = np.round(clf_prod.predict_proba(X_test)[:, 1], 2)\n",
    "preds = pd.DataFrame(preds_array).set_axis(['target'], axis= 'columns') # target probability = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a28ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([df_test_raw.id, preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19605c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>900000</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>900001</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>900002</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>900003</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>900004</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699995</th>\n",
       "      <td>1599995</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699996</th>\n",
       "      <td>1599996</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699997</th>\n",
       "      <td>1599997</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699998</th>\n",
       "      <td>1599998</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699999</th>\n",
       "      <td>1599999</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  target\n",
       "0        900000    0.94\n",
       "1        900001    0.94\n",
       "2        900002    0.00\n",
       "3        900003    0.07\n",
       "4        900004    0.95\n",
       "...         ...     ...\n",
       "699995  1599995    0.58\n",
       "699996  1599996    0.98\n",
       "699997  1599997    0.37\n",
       "699998  1599998    0.08\n",
       "699999  1599999    0.01\n",
       "\n",
       "[700000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c538e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
